{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/workspace\n",
      "['/home/workspace/event_data/2018-11-27-events.csv', '/home/workspace/event_data/2018-11-04-events.csv', '/home/workspace/event_data/2018-11-07-events.csv', '/home/workspace/event_data/2018-11-09-events.csv', '/home/workspace/event_data/2018-11-19-events.csv', '/home/workspace/event_data/2018-11-05-events.csv', '/home/workspace/event_data/2018-11-22-events.csv', '/home/workspace/event_data/2018-11-16-events.csv', '/home/workspace/event_data/2018-11-26-events.csv', '/home/workspace/event_data/2018-11-24-events.csv', '/home/workspace/event_data/2018-11-29-events.csv', '/home/workspace/event_data/2018-11-15-events.csv', '/home/workspace/event_data/2018-11-20-events.csv', '/home/workspace/event_data/2018-11-06-events.csv', '/home/workspace/event_data/2018-11-18-events.csv', '/home/workspace/event_data/2018-11-21-events.csv', '/home/workspace/event_data/2018-11-10-events.csv', '/home/workspace/event_data/2018-11-23-events.csv', '/home/workspace/event_data/2018-11-02-events.csv', '/home/workspace/event_data/2018-11-28-events.csv', '/home/workspace/event_data/2018-11-03-events.csv', '/home/workspace/event_data/2018-11-13-events.csv', '/home/workspace/event_data/2018-11-30-events.csv', '/home/workspace/event_data/2018-11-12-events.csv', '/home/workspace/event_data/2018-11-01-events.csv', '/home/workspace/event_data/2018-11-14-events.csv', '/home/workspace/event_data/2018-11-25-events.csv', '/home/workspace/event_data/2018-11-08-events.csv', '/home/workspace/event_data/2018-11-17-events.csv', '/home/workspace/event_data/2018-11-11-events.csv']\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "file_paths = []\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "    # join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    file_paths.append(file_path_list)\n",
    "    \n",
    "# Remove checkpoint csv file\n",
    "file_paths = file_paths[0]\n",
    "file_paths = [x for x in file_paths if 'checkpoint' not in x]\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workspace/event_data/2018-11-27-events.csv\n",
      "/home/workspace/event_data/2018-11-04-events.csv\n",
      "/home/workspace/event_data/2018-11-07-events.csv\n",
      "/home/workspace/event_data/2018-11-09-events.csv\n",
      "/home/workspace/event_data/2018-11-19-events.csv\n",
      "/home/workspace/event_data/2018-11-05-events.csv\n",
      "/home/workspace/event_data/2018-11-22-events.csv\n",
      "/home/workspace/event_data/2018-11-16-events.csv\n",
      "/home/workspace/event_data/2018-11-26-events.csv\n",
      "/home/workspace/event_data/2018-11-24-events.csv\n",
      "/home/workspace/event_data/2018-11-29-events.csv\n",
      "/home/workspace/event_data/2018-11-15-events.csv\n",
      "/home/workspace/event_data/2018-11-20-events.csv\n",
      "/home/workspace/event_data/2018-11-06-events.csv\n",
      "/home/workspace/event_data/2018-11-18-events.csv\n",
      "/home/workspace/event_data/2018-11-21-events.csv\n",
      "/home/workspace/event_data/2018-11-10-events.csv\n",
      "/home/workspace/event_data/2018-11-23-events.csv\n",
      "/home/workspace/event_data/2018-11-02-events.csv\n",
      "/home/workspace/event_data/2018-11-28-events.csv\n",
      "/home/workspace/event_data/2018-11-03-events.csv\n",
      "/home/workspace/event_data/2018-11-13-events.csv\n",
      "/home/workspace/event_data/2018-11-30-events.csv\n",
      "/home/workspace/event_data/2018-11-12-events.csv\n",
      "/home/workspace/event_data/2018-11-01-events.csv\n",
      "/home/workspace/event_data/2018-11-14-events.csv\n",
      "/home/workspace/event_data/2018-11-25-events.csv\n",
      "/home/workspace/event_data/2018-11-08-events.csv\n",
      "/home/workspace/event_data/2018-11-17-events.csv\n",
      "/home/workspace/event_data/2018-11-11-events.csv\n"
     ]
    }
   ],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_paths:\n",
    "\n",
    "    # Print files processed\n",
    "    print(f)\n",
    "    \n",
    "    # reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile:\n",
    "        \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    "         # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "    \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "# print(len(full_data_rows_list))\n",
    "\n",
    "# uncomment the code below if you would like to check to see what the list of event data rows will look like\n",
    "# print(full_data_rows_list)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    \n",
    "    # Write all the rows in the seperate csvs into the event_datafile_new.csv\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating a Cluster & connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "\n",
    "# To establish connection and begin executing queries, need a session\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a Keyspace \n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set KEYSPACE to the keyspace specified above\n",
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now we will need to create tables to run the following queries. Recall that with Apache Cassandra, you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 1. Requirements: Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "We will model the table after the following query: \n",
    "`SELECT artist, song, length FROM song_session WHERE session_id = 338\n",
    "    AND item_in_session = 4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Declare CREATE and DROP statements\n",
    "create_table_1_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS song_session (\n",
    "        session_id int, item_in_session int, \n",
    "        artist text, \n",
    "        length float, song text,\n",
    "        PRIMARY KEY (session_id, item_in_session)\n",
    "        )\n",
    "\"\"\"\n",
    "# NTS: Cassandra does not have VARCHAR(2)\n",
    "\n",
    "drop_table_1_query = \"\"\"\n",
    "    DROP TABLE IF EXISTS song_session\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create table\n",
    "try:\n",
    "    session.execute(create_table_1_query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# INSERT data into table\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    \n",
    "    for line in csvreader:\n",
    "        query = \"\"\"\n",
    "            INSERT INTO song_session (\n",
    "                session_id, item_in_session, \n",
    "                artist, length, song\n",
    "                )\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        try: \n",
    "            session.execute(query, ( int(line[8]), int(line[3]), line[0], float(line[5]), line[9]) )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Query Description: In this query, I used session_id and item_in_session as the composite primary key. Each row is uniquely identified by a combination of session_id + item_in_session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Faithless', song='Music Matters (Mark Knight Dub)', length=495.30731201171875)\n"
     ]
    }
   ],
   "source": [
    "select_query_1 = \"\"\"\n",
    "    SELECT artist, song, length FROM song_session WHERE session_id = 338\n",
    "    AND item_in_session = 4\n",
    "\"\"\"    \n",
    "\n",
    "try:\n",
    "    rows = session.execute(select_query_1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# 2. Requirements: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "We will model the table with the following query: \n",
    "`SELECT artist, song, first_name, last_name FROM song_playlist_session WHERE session_id = 182\n",
    "    AND user_id = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Declare CREATE and DROP statements\n",
    "create_table_2_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS song_playlist_session (\n",
    "        session_id int,  user_id int, item_in_session int, \n",
    "        artist text, first_name varchar, last_name varchar, \n",
    "        song text,\n",
    "        PRIMARY KEY ((session_id, user_id), item_in_session)\n",
    "        )\n",
    "    WITH CLUSTERING ORDER BY (item_in_session ASC)\n",
    "\"\"\"\n",
    "\n",
    "drop_table_2_query = \"\"\"\n",
    "    DROP TABLE IF EXISTS song_playlist_session\n",
    "\"\"\"              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CREATE TABLE\n",
    "try:\n",
    "    session.execute(create_table_2_query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# INSERT data into table\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    \n",
    "    for line in csvreader:\n",
    "        query = \"\"\"\n",
    "            INSERT INTO song_playlist_session (\n",
    "                session_id,  user_id, item_in_session, \n",
    "                artist, first_name, last_name, \n",
    "                song\n",
    "                )\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            session.execute(query, (int(line[8]), int(line[10]), int(line[3]), line[0], line[1], line[4], line[9]) )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Query Description: In this query, I used session_id and item_in_session as the partition keys, as these 2 can uniquely identify each row. item_in_session is used as a clustering column. Within each partition, they are sorted by item_in_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Down To The Bone', song=\"Keep On Keepin' On\", first_name='Sylvie', last_name='Cruz')\n",
      "Row(artist='Three Drives', song='Greece 2000', first_name='Sylvie', last_name='Cruz')\n",
      "Row(artist='Sebastien Tellier', song='Kilometer', first_name='Sylvie', last_name='Cruz')\n",
      "Row(artist='Lonnie Gordon', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', first_name='Sylvie', last_name='Cruz')\n"
     ]
    }
   ],
   "source": [
    "select_query_2 = \"\"\"\n",
    "    SELECT artist, song, first_name, last_name FROM song_playlist_session WHERE session_id = 182\n",
    "    AND user_id = 10\n",
    "\"\"\"    \n",
    "\n",
    "try:\n",
    "    rows = session.execute(select_query_2)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# 3.Requirements: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "We will model the table after the query: \n",
    "`SELECT first_name, last_name FROM user_song WHERE song = 'All Hands Against His Own'`\n",
    "\n",
    "`PRIMARY KEY is declared ((song), user_id)`. song is the partition key, user_id is also used in the primary key and is the clustering column\n",
    "\n",
    "user_id is used as it can be used to uniquely identify each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Declare CREATE and DROP statements\n",
    "create_table_3_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user_song (\n",
    "        song text, user_id int, \n",
    "        first_name varchar, last_name varchar, \n",
    "        PRIMARY KEY ((song), user_id)\n",
    "        )\n",
    "\"\"\"\n",
    "\n",
    "drop_table_3_query = \"\"\"\n",
    "    DROP TABLE IF EXISTS user_song\n",
    "\"\"\"                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CREATE TABLE\n",
    "try:\n",
    "    session.execute(create_table_3_query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# INSERT data into table\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    \n",
    "    for line in csvreader:\n",
    "        query = \"\"\"\n",
    "            INSERT INTO user_song (\n",
    "                song, user_id,\n",
    "                first_name, last_name\n",
    "                )\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        try: \n",
    "            session.execute(query, (line[9], int(line[10]), line[1], line[4]) )                     \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Query Description: In this query, I used song and user_id as the primary keys, as these 2 can uniquely identify each row. song is used as the PARTITION KEY while user_id is used as the CLUSTERING COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(first_name='Jacqueline', last_name='Lynch')\n",
      "Row(first_name='Tegan', last_name='Levine')\n",
      "Row(first_name='Sara', last_name='Johnson')\n"
     ]
    }
   ],
   "source": [
    "select_query_3 = \"\"\"\n",
    "    SELECT first_name, last_name FROM user_song WHERE song = 'All Hands Against His Own'\n",
    "\"\"\"    \n",
    "\n",
    "try:\n",
    "    rows = session.execute(select_query_3)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(drop_table_1_query)\n",
    "    session.execute(drop_table_2_query)\n",
    "    session.execute(drop_table_3_query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
