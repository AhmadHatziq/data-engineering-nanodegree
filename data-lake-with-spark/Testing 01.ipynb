{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Notebook 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count statistics\n",
    "stats = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract AWS keys from config file & store into environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config.get('AWS','KEY')\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config.get('AWS','SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define schemas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl\n",
    "from pyspark.sql.types import StringType as Str, IntegerType as Int, DateType as Date, LongType as Long\n",
    "\n",
    "song_schema = R([\n",
    "    Fld(\"artist_id\", Str()),\n",
    "    Fld(\"artist_latitude\", Dbl()),\n",
    "    Fld(\"artist_location\", Str()),\n",
    "    Fld(\"artist_longitude\", Dbl()),\n",
    "    Fld(\"artist_name\", Str()),\n",
    "    Fld(\"duration\", Dbl()),\n",
    "    Fld(\"num_songs\", Long()),\n",
    "    Fld(\"song_id\", Str()),\n",
    "    Fld(\"title\", Str()),\n",
    "    Fld(\"year\", Long())\n",
    "])\n",
    "\n",
    "log_schema = R([\n",
    "    Fld(\"artist\", Str()),\n",
    "    Fld(\"auth\", Str()),\n",
    "    Fld(\"firstName\", Str()),\n",
    "    Fld(\"gender\", Str()),\n",
    "    Fld(\"itemInSession\", Long()),\n",
    "    Fld(\"lastName\", Str()),\n",
    "    Fld(\"length\", Dbl()),\n",
    "    Fld(\"level\", Str()),\n",
    "    Fld(\"location\", Str()),\n",
    "    Fld(\"method\", Str()),\n",
    "    Fld(\"page\", Str()),\n",
    "    Fld(\"registration\", Dbl()),\n",
    "    Fld(\"sessionId\", Long()),\n",
    "    Fld(\"song\", Str()),\n",
    "    Fld(\"status\", Long()),\n",
    "    Fld(\"ts\", Long()),\n",
    "    Fld(\"userAgent\", Str()),\n",
    "    Fld(\"userId\", Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load schema from S3\n",
    "\n",
    "Access the public s3 bucket from http via: https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare S3 locations\n",
    "s3_song = \"s3://udacity-dend/song_data\"\n",
    "s3_log = \"s3://udacity-dend/log_data\"\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3://udacity-dend-project-output-1995/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://udacity-dend/log-data/*/*/*events.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create filepath for log data\n",
    "log_data_folder = os.path.join(input_data, \"log-data/\")\n",
    "log_files = \"{}*/*/*events.json\".format(log_data_folder)\n",
    "log_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://udacity-dend/song-data/*/*/*/*.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create filepath for song data\n",
    "song_data_folder = os.path.join(input_data, \"song-data/\")\n",
    "song_files = \"{}*/*/*/*.json\".format(song_data_folder)\n",
    "song_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song files:  s3a://udacity-dend/song-data/A/A/A/*.json\n"
     ]
    }
   ],
   "source": [
    "# read song data file. NTS we have a lot of song data. \n",
    "# So only select a small amount for testing purposes.\n",
    "song_files = \"{}A/A/A/*.json\".format(song_data_folder) # Select a small subset of the data\n",
    "# song_files = \"{}*/*/*/*.json\".format(song_data_folder) # Select all the data\n",
    "# song_files = \"{}A/A/*/*.json\".format(song_data_folder) # Only select files in the first A folder\n",
    "print(\"Song files: \", song_files)\n",
    "df_song = spark.read.json(song_files, schema = song_schema).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log data file\n",
    "df_log = spark.read.json(log_files, schema = log_schema).dropDuplicates().cache()\n",
    "\n",
    "# Filter by nextSong action\n",
    "df_log = df_log.filter(df_log.page == \"NextSong\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song count:  24\n",
      "Log count:  6820\n"
     ]
    }
   ],
   "source": [
    "print(\"Song count: \", df_song.count())\n",
    "print(\"Log count: \", df_log.count())\n",
    "\n",
    "stats[\"df_log size\"] = df_log.count()\n",
    "stats[\"df_song size\"] = df_song.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "+------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|            artist|     auth|firstName|gender|itemInSession| lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|           Fat Joe|Logged In|     Kate|     F|           21|  Harrell|241.34485| paid|Lansing-East Lans...|   PUT|NextSong|1.540472624796E12|      605|Safe 2 Say [The I...|   200|1542296032796|\"Mozilla/5.0 (X11...|    97|\n",
      "|       Linkin Park|Logged In|     Kate|     F|           33|  Harrell|259.86567| paid|Lansing-East Lans...|   PUT|NextSong|1.540472624796E12|      605|         My December|   200|1542299023796|\"Mozilla/5.0 (X11...|    97|\n",
      "|     The Saturdays|Logged In|    Chloe|     F|           20|   Cuevas|176.95302| paid|San Francisco-Oak...|   PUT|NextSong|1.540940782796E12|      630|     If This Is Love|   200|1542318319796|Mozilla/5.0 (Wind...|    49|\n",
      "|       Wim Mertens|Logged In|   Aleena|     F|           71|    Kirby|240.79628| paid|Waterloo-Cedar Fa...|   PUT|NextSong|1.541022995796E12|      619|          Naviamente|   200|1542321121796|Mozilla/5.0 (Maci...|    44|\n",
      "|The Avett Brothers|Logged In| Mohammad|     M|            1|Rodriguez| 271.0722| paid|Sacramento--Rosev...|   PUT|NextSong|1.540511766796E12|      744|   The Perfect Space|   200|1542786093796|\"Mozilla/5.0 (Mac...|    88|\n",
      "+------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the df schemas\n",
    "df_log.printSchema()\n",
    "df_log.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|             title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+------------------+----+\n",
      "|ARJNIUY12298900C91|           null|                    |            null|        Adelitas Way| 213.9424|        1|SOBLFFE12AF72AA5BA|            Scream|2009|\n",
      "|AR10USD1187B99F3F1|           null|Burlington, Ontar...|            null|Tweeterfriendly M...|189.57016|        1|SOHKNRJ12A6701D1F8|      Drop of Rain|   0|\n",
      "|AR0MWD61187B9B2B12|           null|                    |            null|International Noi...|195.39546|        1|SOHOZBI12A8C132E3C|       Smash It Up|2000|\n",
      "|ARCLYBR1187FB53913|       37.54703|       San Mateo, CA|      -122.31483|          Neal Schon|304.56118|        1|SOOVHYF12A8C134892|   I'll Be Waiting|1989|\n",
      "|ARGE7G11187FB37E05|           null|        Brooklyn, NY|            null|        Cyndi Lauper|240.63955|        1|SONRWUU12AF72A4283|Into The Nightlife|2008|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the df schemas\n",
    "df_song.printSchema()\n",
    "df_song.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTS: Import whole data using the above schema\n",
    "\n",
    "## Recall the flow of data\n",
    "- Table: time\n",
    "    - start_time - ts from df_log\n",
    "    - hour - transform from start_time\n",
    "    - day - transform from start_time\n",
    "    - week - transform from start_time\n",
    "    - month - transform from start_time\n",
    "    - year - transform from start_time\n",
    "- Table: users \n",
    "    - user_id - userId from df_log\n",
    "    - first_name - firstName from df_logs\n",
    "    - last_name - lastName from df_logs \n",
    "    - gender - gender from df_logs\n",
    "    - level - level from df_logs\n",
    "- Table: artists\n",
    "    - artist_id - artist_id from df_song\n",
    "    - name - artist_name from df_song\n",
    "    - location - artist_location from df_song\n",
    "    - lattitude - artist_latitude from df_song\n",
    "    - longitude - artst_longitude from df_song\n",
    "- Table: songs\n",
    "    - song_id - song_id from df_song\n",
    "    - title - title from df_song\n",
    "    - artist_id - artist_id from df_song\n",
    "    - year - year from df_song\n",
    "    - duration - duration from df_song\n",
    "- Table: songplays\n",
    "    - songplay_id -\n",
    "    - start_time -\n",
    "    - user_id -\n",
    "    - level -\n",
    "    - song_id -\n",
    "    - artist_id - \n",
    "    - session_id - \n",
    "    - location - \n",
    "    - user_agent - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table: time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           ts|\n",
      "+-------------+\n",
      "|1541106106796|\n",
      "|1541106352796|\n",
      "|1541106496796|\n",
      "|1541106673796|\n",
      "|1541107053796|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_log.select(\"ts\").dropDuplicates().sort(\"ts\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 15:33:52.796000\n"
     ]
    }
   ],
   "source": [
    "# Test a function for parsing to datetime\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Input string is of type long. But since long doesn't exist in Python, use int.\n",
    "sample_time_string_1 = int(1542296032796)\n",
    "sample_time_string_2 = int(1541106496796)  \n",
    "\n",
    "def convert_to_datetime(text):\n",
    "    \n",
    "    obj = datetime.fromtimestamp(text / 1000)\n",
    "    return obj\n",
    "\n",
    "datetime_obj = (convert_to_datetime(sample_time_string_1))\n",
    "print(datetime_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain the time stamp column\n",
    "df_time = df_log.select(\"ts\")\n",
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "df_time = df_time.withColumn(\"start_time\", get_timestamp(\"ts\"))\n",
    "# df_time = df_time.drop(\"ts\")\n",
    "df_time.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+-------------+--------------------+----+---+-----+----+----+-------+\n",
      "|           ts|          start_time|hour|day|month|year|week|weekday|\n",
      "+-------------+--------------------+----+---+-----+----+----+-------+\n",
      "|1542296032796|2018-11-15 15:33:...|  15| 15|   11|2018|  46|      5|\n",
      "|1542299023796|2018-11-15 16:23:...|  16| 15|   11|2018|  46|      5|\n",
      "|1542318319796|2018-11-15 21:45:...|  21| 15|   11|2018|  46|      5|\n",
      "|1542321121796|2018-11-15 22:32:...|  22| 15|   11|2018|  46|      5|\n",
      "|1542786093796|2018-11-21 07:41:...|   7| 21|   11|2018|  47|      4|\n",
      "+-------------+--------------------+----+---+-----+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the other columns: hour, day, week, month, year, weekday\n",
    "from pyspark.sql.functions import hour, dayofmonth, month, year, weekofyear, dayofweek\n",
    "\n",
    "df_time = df_time.withColumn(\"hour\", hour(\"start_time\"))\n",
    "df_time = df_time.withColumn(\"day\", dayofmonth(\"start_time\"))\n",
    "df_time = df_time.withColumn(\"month\", month(\"start_time\"))\n",
    "df_time = df_time.withColumn(\"year\", year(\"start_time\"))\n",
    "df_time = df_time.withColumn(\"week\", weekofyear(\"start_time\"))\n",
    "df_time = df_time.withColumn(\"weekday\", dayofweek(\"start_time\"))\n",
    "\n",
    "df_time.printSchema()\n",
    "df_time.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- datetime: date (nullable = true)\n",
      "\n",
      "+-------------+--------------------+----+---+-----+----+----+-------+-----------+\n",
      "|           ts|          start_time|hour|day|month|year|week|weekday|   datetime|\n",
      "+-------------+--------------------+----+---+-----+----+----+-------+-----------+\n",
      "|1542296032796|2018-11-15 15:33:...|  15| 15|   11|2018|  46|      5|50843-06-01|\n",
      "|1542299023796|2018-11-15 16:23:...|  16| 15|   11|2018|  46|      5|50843-07-06|\n",
      "|1542318319796|2018-11-15 21:45:...|  21| 15|   11|2018|  46|      5|50844-02-14|\n",
      "|1542321121796|2018-11-15 22:32:...|  22| 15|   11|2018|  46|      5|50844-03-17|\n",
      "|1542786093796|2018-11-21 07:41:...|   7| 21|   11|2018|  47|      4|50858-12-11|\n",
      "+-------------+--------------------+----+---+-----+----+----+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a datetime column\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "df_time = df_time.withColumn('datetime', f.to_date(df_time.ts.cast(dataType=t.TimestampType())))\n",
    "df_time.printSchema()\n",
    "df_time.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct rows only\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "time_table = df_time.select(col(\"start_time\"), col(\"hour\"), col(\"day\"), col(\"week\"), \\\n",
    "                           col(\"month\"), col(\"year\"), col(\"weekday\")).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the parquet file to local storage instead since cannot access S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workspace/parquet_output/time.parquet'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder = \"/home/workspace/parquet_output\"\n",
    "output = os.path.join(output_folder, 'time.parquet')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.partitionBy(\"year\", \"month\").parquet(output, 'overwrite')\n",
    "\n",
    "stats[\"df_time\"] = time_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table: users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "+---------+--------+------+-----+------+\n",
      "|firstName|lastName|gender|level|userId|\n",
      "+---------+--------+------+-----+------+\n",
      "|Katherine|     Gay|     F| free|    57|\n",
      "|  Shakira|    Hunt|     F| free|    84|\n",
      "|     Sean|  Wilson|     F| free|    22|\n",
      "| Theodore|   Smith|     M| free|    52|\n",
      "|    Tegan|  Levine|     F| paid|    80|\n",
      "+---------+--------+------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract user data\n",
    "df_users = df_log.select(col(\"firstName\"), col(\"lastName\"), col(\"gender\"), col(\"level\"), col(\"userId\")).distinct()\n",
    "df_users.printSchema()\n",
    "df_users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output path for user parquet\n",
    "output = os.path.join(output_folder, 'users.parquet')\n",
    "\n",
    "# write user table\n",
    "df_users.write.parquet(output, 'overwrite')\n",
    "\n",
    "stats[\"df_users\"] = df_users.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table: artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      "\n",
      "+------------------+-------------+---------------+----------------+---------------+\n",
      "|         artist_id|  artist_name|artist_location|artist_longitude|artist_latitude|\n",
      "+------------------+-------------+---------------+----------------+---------------+\n",
      "|ARC1IHZ1187FB4E920| Jamie Cullum|               |            null|           null|\n",
      "|ARZKCQM1257509D107|   Dataphiles|               |            null|           null|\n",
      "|AREWD471187FB49873|     Son Kite|               |            null|           null|\n",
      "|ARGE7G11187FB37E05| Cyndi Lauper|   Brooklyn, NY|            null|           null|\n",
      "|ARSVTNL1187B992A91|Jonathan King|London, England|        -0.12714|       51.50632|\n",
      "+------------------+-------------+---------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select artists columns\n",
    "df_artists = df_song.select(col(\"artist_id\"), col(\"artist_name\"), col(\"artist_location\"), \\\n",
    "                               col(\"artist_longitude\"), col(\"artist_latitude\")).distinct()\n",
    "\n",
    "df_artists.printSchema()\n",
    "df_artists.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output path for artist parquet file\n",
    "output = os.path.join(output_folder, 'artist.parquet')\n",
    "\n",
    "# write artist table\n",
    "df_artists.write.parquet(output, 'overwrite')\n",
    "\n",
    "stats[\"df_artists\"] = df_artists.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table: Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SODZYPO12A8C13A91E|Burn My Body (Alb...|AR1C2IX1187B99BF74|   0|177.99791|\n",
      "|SOIGHOD12A8C13B5A1|        Indian Angel|ARY589G1187B9A9F4E|2004|171.57179|\n",
      "|SOOVHYF12A8C134892|     I'll Be Waiting|ARCLYBR1187FB53913|1989|304.56118|\n",
      "|SOAPERH12A58A787DC|The One And Only ...|ARZ5H0P1187B98A1DD|   0|230.42567|\n",
      "|SOHKNRJ12A6701D1F8|        Drop of Rain|AR10USD1187B99F3F1|   0|189.57016|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select song columns\n",
    "df_songs_table = df_song.select(col(\"song_id\"), col(\"title\"), col(\"artist_id\"), col(\"year\"), col(\"duration\")).distinct()\n",
    "\n",
    "df_songs_table.printSchema()\n",
    "df_songs_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output path for songs parquet file\n",
    "output = os.path.join(output_folder, 'song.parquet')\n",
    "\n",
    "# write songs table partition by year and artists\n",
    "df_songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(output, 'overwrite')\n",
    "\n",
    "stats[\"df_songs_table\"] = df_songs_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table: songplays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the SQL table as we have to run queries against it\n",
    "df_song.createOrReplaceTempView(\"song_table_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the df_time table as we require the timestamp column\n",
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "df_time = df_log\n",
    "df_time = df_time.withColumn(\"start_time\", get_timestamp(\"ts\"))\n",
    "df_time = df_time.withColumn(\"month\", month(\"start_time\"))\n",
    "df_time = df_time.withColumn(\"year\", year(\"start_time\"))\n",
    "df_time.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique song id, artist id, artist name\n",
    "song_df = spark.sql(\"SELECT DISTINCT song_id, artist_id, artist_name FROM song_table_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_table = df_time.join(song_df, song_df.artist_name == df_time.artist, \"inner\").distinct()\n",
    "temp_table.printSchema()\n",
    "temp_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_songplays = temp_table.select([\"start_time\", \"userId\", \"level\", \"song_id\", \"artist_id\", \"sessionId\", \"location\", \"userAgent\", \"month\", \"year\"])\n",
    "\n",
    "df_songplays.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the id column\n",
    "df_songplays = df_songplays.withColumn(\"songplay_id\", monotonically_increasing_id()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workspace/parquet_output/songplays.parquet'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get output path for songplays parquet\n",
    "output = os.path.join(output_folder, 'songplays.parquet')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write user table\n",
    "df_songplays.write.partitionBy(\"year\", \"month\").parquet(output, 'overwrite')\n",
    "\n",
    "stats[\"df_songplays\"] = df_songplays.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVerall statistics: \n",
      "{'df_log size': 6820, 'df_song size': 24, 'df_time': 6813, 'df_users': 104, 'df_artists': 24, 'df_songs_table': 24, 'df_songplays': 10}\n"
     ]
    }
   ],
   "source": [
    "print(\"OVerall statistics: \")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read parquet file of song table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_parquet_filepath = os.path.join(output_folder, 'song.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(song_parquet_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------+----+------------------+\n",
      "|           song_id|               title| duration|year|         artist_id|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|SOBTCUI12A8AE48B70|Faust: Ballet Mus...| 94.56281|   0|ARSUVLW12454A4C8B8|\n",
      "|SOVNKJI12A8C13CB0D|Take It To Da Hou...|227.10812|2001|ARWUNH81187FB4A3E0|\n",
      "|SOYVBGZ12A6D4F92A8|Piano Sonata No. ...|221.70077|   0|ARLRWBW1242077EB29|\n",
      "|SODBHKO12A58A77F36|Fingers Of Love (...|335.93424|   0|ARKGS2Z1187FB494B5|\n",
      "|SOGXFIF12A58A78CC4|Hanging On (Mediu...|204.06812|   0|AR5LZJD1187FB4C5E5|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in song and artist data to use for songplays table\n",
    "song_parquet_filepath = os.path.join(output_folder, 'song.parquet')\n",
    "song_df = spark.read.parquet(song_parquet_filepath)\n",
    "\n",
    "artist_parquet_filepath = os.path.join(output_folder, 'artist.parquet')\n",
    "artist_df = spark.read.parquet(artist_parquet_filepath)\n",
    "\n",
    "song_df.printSchema()\n",
    "artist_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      "\n",
      "+------------------+------------------+--------------------+---------+----+----------------+---------------+----------------+---------------+\n",
      "|         artist_id|           song_id|               title| duration|year|     artist_name|artist_location|artist_longitude|artist_latitude|\n",
      "+------------------+------------------+--------------------+---------+----+----------------+---------------+----------------+---------------+\n",
      "|ARGE7G11187FB37E05|SONRWUU12AF72A4283|  Into The Nightlife|240.63955|2008|    Cyndi Lauper|   Brooklyn, NY|            null|           null|\n",
      "|ARBZIN01187FB362CC|SOERIDA12A6D4F8506|I Want You (Album...|192.28689|2006|    Paris Hilton|             27|       103.78871|        1.32026|\n",
      "|ARKYKXP11F50C47A6A|SONQPZK12AB0182D84|         Double Wide|160.20853|   0|The Supersuckers|               |            null|           null|\n",
      "|ARY589G1187B9A9F4E|SOIGHOD12A8C13B5A1|        Indian Angel|171.57179|2004|     Talkdemonic|   Portland, OR|      -122.67563|       45.51179|\n",
      "|ARSVTNL1187B992A91|SOEKAZG12AB018837E|I'll Slap Your Fa...|129.85424|2001|   Jonathan King|London, England|        -0.12714|       51.50632|\n",
      "+------------------+------------------+--------------------+---------+----+----------------+---------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the song and artist df\n",
    "temp_table = song_df.join(artist_df, ['artist_id'], \"inner\").distinct()\n",
    "temp_table.printSchema()\n",
    "temp_table.count()\n",
    "temp_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_table.createOrReplaceTempView(\"song_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_song = spark.sql(\"SELECT DISTINCT song_id, artist_id, artist_name FROM song_table\")\n",
    "unique_song.printSchema()\n",
    "unique_song.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test writing output file to my own bucket\n",
    "Access my s3 bucket from http via: https://s3.console.aws.amazon.com/s3/buckets/udacity-dend-project-output-1995/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8E116C0B02D2ABC4',\n",
       "  'HostId': '7992db/s2B+SPF+4ILkUrZj3Pb+JH5BEMF2TgPBTSS+r5w/NsayqZYLEqfBSyINpeWjRWYI0/8JW4m1KnB2McA==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '7992db/s2B+SPF+4ILkUrZj3Pb+JH5BEMF2TgPBTSS+r5w/NsayqZYLEqfBSyINpeWjRWYI0/8JW4m1KnB2McA==',\n",
       "   'x-amz-request-id': '8E116C0B02D2ABC4',\n",
       "   'date': 'Thu, 21 Jan 2021 05:39:13 GMT',\n",
       "   'etag': '\"4a4ba548fe7ddb965593f41a13e1df90\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"4a4ba548fe7ddb965593f41a13e1df90\"'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    region_name='us-east-2',\n",
    "    aws_access_key_id = os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    ")\n",
    "\n",
    "content=\"String content to write to a new S3 file\"\n",
    "s3.Object('udacity-dend-project-output-1995', 'newfile.txt').put(Body=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
